# 自注意力和编解码器注意力

## 自注意力 Self-Attention

自注意力指的是编码器内部或解码器内部，计算自己内部的注意力值

这就意味着 QKV 都来自于编解码器自己内部

例如: 编码器内部的自注意力称为 Encoder Self-Attention；解码器内部自注意力称为 Decoder Self-Attention

## 编解码器注意力 Encoder-Decoder Attention

Q 来自于解码器，KV 来自于编码器，这样查询出来的结果我们称为 Encoder-Decoder Attention

## 总结

注意力算法需要用到 QKV 矩阵，那么我们可以根据 QKV 矩阵的来源来区分自注意力和编解码注意力

- QKV 来自于编码器或解码器自身，则为 自注意力
- Q 来自于解码器 KV 来自于编码器，则为 编解码注意力