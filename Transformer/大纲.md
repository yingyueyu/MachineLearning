# 大纲

- ChatGPT的发展与技术栈
- Encoder-Decoder
- 注意力机制
  - (todo) [Nadaraya-Watson 回归进行注意力池](https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html#attention-pooling-via-nadarayawatson-regression) (原文中的图像证明了，通过注意力可以预测出回归函数，并与真实的回归结果接近)
  - (todo) [通过调整$\sigma$系数，用来调整高斯核函数](https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html#adapting-attention-pooling)
- 位置编码
- Transformer
- 语音场景与认识声音
  - 语音的认知
  - 声音的频域变换
  - 声音的梅尔特征
- 大模型的部署与微调
  - 大语言模型介绍
    - GPT
    - BERT
    - T5
    - GLM
    - Llama
  - 大模型微调方案
  - LLM中的Attention优化
  - NLP评价指标