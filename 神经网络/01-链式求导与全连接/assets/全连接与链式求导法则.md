## 前向传播回顾

由此前一元线性回归案例中的模型可知，我们需要拟合公式为：

$$
y = wx + b
$$

且我们可以设计其损失函数（采用均方差方式）：

$$
e = \frac{1}{n} \sum_{i=1}^{n} (wx_{i} - y_{i})^{2}
$$

令b = 0，则e关于w参数的近似函数:

$$
e = aw^{2} - bw + c
$$

图像:

![图片描述](img/deep_2023-10-27_09-50-15.png)

在最低点，我们可知便是方差最小的位置，由此可知：

带入不同的w和b的值很有可能就会得到最小方差

---

## 反向传播回顾

我们试着给定一个随机的w（b=0），那么它在图中便会标注出一个点


![图片描述](img/deep_2023-10-27_10-15-30.png)

由方差的原始函数可知

$$
e = [y - (wx + b)]_{2}
$$

我们为其设置一个复合函数(此时并非高数理论):

$$
\begin{cases}
e = [y - a]_{2} \\
a = wx + b
\end{cases}
$$

根据求导后的公式可知：

$$
\frac{de}{da}=-2(y-a) \\
\frac{da}{dw}=x \\
\frac{da}{db}=1 \\
$$

由乘法可得：

$$
\frac{de}{dw}=-2(y-a)x \\
\frac{de}{db}=-2(y-a)
$$

对于w与b来说，他们的值分别与方差e呈现出凹曲线。
这一点我们可以利用方差公式可知。

![图片描述](img/deep_2023-10-27_13-59-53.png)

当w与b的点位于整个曲面最底部时，我们就认为得到了可以使方差最小的点
因此我们可以知道，求出曲面底部点的位置坐标，是我们的首要任务。

求最佳的w和b，我们可以使用减法的方式，通过一次次的减运算，我们可以得到合适的值
但对于减运算该如何进行呢，有如下三种方式：

+ 方式一：减小固定值

![图片描述](img/deep_2023-10-27_14-53-01.png)

从上图可知，减固定值，虽然可以使w值降低，但有可能会错过最佳w，无法精确查找，有可能有偏大或者偏小的倾向

+ 方式二：减斜率

![图片描述](img/deep_2023-10-27_14-58-33.png)

从上图可知，减斜率，可以使w值稳步下降，但问题是斜率变化不大的情况下，递减次数也会非常大，下降十分缓慢

+ 方式三：减（斜率*小固定值）

![图片描述](img/deep_2023-10-27_15-08-54.png)

从上图可知，减去 斜率乘以小固定值，就可以让斜率的查找更加合理。在斜率大的地方加速递减，在斜率小的地方减缓递减。
由于固定值足够小，因此递减过程也不会太大。

---

## 模拟反向传播过程

```python
import matplotlib.pyplot as plt
from IPython import display
import numpy as np
import time


def loss_function(w, b, x_data, y_data):
    # 预测对应的 y 的值
    y_predicted = w * x_data + b  # 使用训练得到的 w 和 b 预测 y 值
    # 计算均方误差
    e_bar = np.mean((y_data - y_predicted) ** 2)
    return e_bar


# 定义输入数据
data = [[0.8, 1.0], [1.7, 0.9], [2.7, 2.4], [3.2, 2.9], [3.7, 2.8], [4.2, 3.8], [4.2, 2.7]]
# 转换为 NumPy 数组
data = np.array(data)
# 提取 x_data 和 y_data
x_data = data[:, 0]
y_data = data[:, 1]

# 计算线性回归直线的两个端点
w = 1.9
b = 0

# 迭代更新 w_old 的值并重新绘制曲线
w_old = w
min_fixed_value = 0.01  # 选项三中使用的小固定值
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))
w_values = np.linspace(-10, 10, 200)  # w 取值范围
loss_values = [(np.mean((y_data - (w * x_data + b)) ** 2)) for w in w_values]  # 计算损失函数的值

num_iterations = 5
for i in range(num_iterations + 1):
    # 清空Jupyter输出
    display.clear_output(wait=True)
    # 清空上次绘制的内容
    ax1.cla()
    ax2.cla()

    # 左侧子图：损失函数曲线和切线
    ax1.set_xlim(-10, 10)
    ax1.set_ylim(-800, 800)
    ax1.set_xlabel("w")
    ax1.set_ylabel("e")
    ax1.set_title("Loss Function")
    ax1.plot(w_values, loss_values, color='g', linewidth=2)  # 绘制损失函数曲线
    ax1.plot(w_old, loss_function(w_old, b, x_data, y_data), marker='o', markersize=8, color='r')  # 标记当前 w 值对应的损失

    # 计算切线斜率和截距
    tangent_point = np.mean((y_data - (w_old * x_data + b)) ** 2)  # 在 w_old 处对应的损失
    tangent_slope = -2 * np.mean(x_data * y_data) + 2 * w_old * np.mean(x_data ** 2)  # 切线的斜率为损失函数在 w_old 处的导数
    tangent_intercept = tangent_point - tangent_slope * w_old  # 切线的截距

    # 绘制切线
    tangent_line = tangent_slope * w_values + tangent_intercept
    ax1.plot(w_values, tangent_line, color='b', linestyle='--')  # 绘制切线

    # 右侧子图：散点和拟合直线
    ax2.set_xlim(0, 7)
    ax2.set_ylim(-15, 15)
    ax2.set_xlabel("x axis label")
    ax2.set_ylabel("y axis label")
    ax2.scatter(x_data, y_data, color='b')  # 绘制训练数据散点图
    y_lower_lr = w_old * 0 + b
    y_upper_lr = w_old * 5 + b
    ax2.plot([0, 7], [y_lower_lr, y_upper_lr], color='r', linewidth=3)  # 绘制线性回归直线
    ax2.set_title("Data Scatter and Fitted Line")

    # 根据不同选项更新 w_old 的值, 减去切线斜率乘以最小固定值
    w_new = w_old - tangent_slope * min_fixed_value
    w_old = w_new

    # 显示图形
    display.display(plt.gcf())
    time.sleep(3)
    # 清空Jupyter输出
    display.clear_output(wait=True)
```

![png](../../torch-base/c-04-全连接神经网络/01前向与反向过程/output_1_0.png)

#### 上述实验的流程

+ 构建原始散点图xy的点数据

+ 为w、b参数声明随机值（此处b=0，未分析b）

+ 计算方差loss_value(也就是前文中的e)，绘制出e关于w的坐标图（左图）

+ 将w_old 记录为w的随机初始值，并绘对应的点于e关于w的坐标图（左图）

+ 根据e对w的导数公式 $ \frac{de}{dw} = -2(y-a)x $ 可获得w_old点斜率，且通过斜率绘制斜截线（左图虚线）

+ 根据w_old 点我们同样可以设计出它此时在xy坐标图中斜截线（右图红线）

+ 根据SGD优化器`y_new = y_old - learning_rate * 小固定值`,我们可以获取下一次的新的斜率

+ 回到方差计算loss_value步骤，直到寻到合适的拟合线

## 激活函数回顾

上述的反向传播案例，我们并不能将它适用于现实中的所有线性回归问题。比如像如下的散点图

![图片描述](img/deep_2023-10-27_15-24-26.png)

由于我们采用的前向传播公式是 $ y = xw + b $, 因此它仅能以一条直线去尽可能拟合曲线

![图片描述](img/deep_2023-10-27_15-31-46.png)

中国有句古话“君子有所为，君子有所不为”， $ y = xw + b $ 能够在直线散点中发挥作用，但
它的拟合方式也仅仅限于直线拟合

而$ y = xw + b $ 作为案例的预测的拟合函数，我们也将其称为“激活函数”

事实上，在研究者的研究中激活函数的正确公式应该为 $ y = wf(x) + b $,
我们认为f(x)可以是认知上的任何函数。例如：sin、cos等等。

但研究者解决广泛问题的激活函数，一定不是简单的数学基础函数

比如：

#### sigmoid

$$ sigmoid=\frac{1}{1+e^{-x}} $$

函数图为：

![图片描述](img/deep_2023-10-27_15-41-10.png)

#### tanh

$$ tanh=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} $$

函数图为：

![图片描述](img/deep_2023-10-27_15-45-10.png)

#### Relu

$$ Relu=max(0,x) $$

函数图为：

![图片描述](img/deep_2023-10-27_15-50-04.png)

#### softmax

$$ softmax=\frac{e^{x}}{\sum_{i=1}^n e^{x}_{i}} $$

函数图为：

![softmax](img/deep_2023-10-30_09-09-19.png)

上述四个激活函数为常见激活函数中的代表，但每一种激活函数应对问题，都必须具有针对性
这一点就好像“好钢用在刀刃上”

而我们要知道哪种激活函数在哪种情况中适用，就必须了解它的求导图。（这里就不再赘述，机器学习中的知识点）
总而言之：

+ sigmoid 主要用于“二分类”、“是否、对错”问题
+ tanh 相较于sigmoid有同样的作用，但其收敛速度更快
+ Relu 主要用于“多分类”问题
+ Softmax 主要用于分析同一场景下的概率问题，例如石头剪刀布、天气等

在采用sigmoid激活函数后，我们便可以通过训练，得到如下结果：

![sigmoid拟合模型](img/deep_2023-10-30_09-28-55.png)

可以从该结果可以看到，拟合方式不再是一条直线，而是完全的曲线。

如何使用tanh激活函数，我们便可以更快的得到拟合曲线

![sigmoid拟合模型](img/deep_2023-10-30_09-32-36.png)

其他激活函数效果在此不再说明

## 全连接

### 为何使用全连接

此时我们可以先不讨论全连接是什么，我们可以先思考一个问题：
如果我们我们采用上述的实验案例的方式，去解决如下问题会有什么结果

![一个周期的sin函数](img/deep_2023-10-30_09-47-53.png)

我们使用

$$
y=w \cdot sigmoid(x) + b
$$

将sigmoid公式带入上述的实验案例中，经过多次训练会得到

![一个周期的sin函数](img/deep_2023-10-30_09-43-55.png)

还记得sigmoid的函数图吗，是一个0到1之间的曲线

![sigmoid函数](img/deep_2023-10-30_09-57-27.png)

它只有一个拐点，所以对于sin函数的两个拐点来说，我们需要的是两个sigmoid函数去拟合这个曲线

![两个sigmoid函数](img/deep_2023-10-30_10-02-51.png)

但是两个sigmoid函数要如何拼接，这里就是我们需要学习的`全连接`

---

### 全连接理解

我们还是以 $ y = wx + b $ 函数为例，我们将这个函数从输入到输出的过程称为一层神经元

![一层神经元](img/deep_2023-10-30_10-51-07.png)

当我们需要构建两层神经元时，我们需要把上一层的y作为下一层的x值进行输入

![两层神经元](img/deep_2023-10-30_10-54-22.png)

由此我们构成了两层神经元，而 x1 的部分又被我们称为“`Hidden Layer`”(隐藏层)

x 在此处作为特征输入，例如输入你的年龄，判断你是否成年，这里的x就是年龄。

而现实问题不可能是上述这种简单的线性问题，例如你的身体BMI指数，是用身高和体重作为特征的；
零件的质量，可能是有零件高度、零件宽度、零件重量等特征因素决定的。

这里我们设x1、x2、x3 分别为零件的高度、宽度、重量。利用神经元图我们可以得到这样一个图

![全连接](img/deep_2023-10-30_11-06-04.png)

图中的y就是我们需要得到的零件的质量

但零件的质量与三个特征之间的关系一定是这样的一层线性关系吗，有没有可能是一个类似sin的函数曲线呢
,而我们需要如何构建隐藏层呢?

我们需要明白核心的几个主要问题：

1、隐藏层决定了什么

2、隐藏层的数量有什么意义

3、隐藏层中张量个数（神经元个数）有什么意义

3、输入、输出如何定义

### 隐藏层决定了什么

在`Cross Validated` 网站中层有一个关于`隐藏层有何作用`的回答：

- 每个图层都可以应用您想要的任何函数到前一层（通常是线性变换，然后是压缩非线性）。

- 隐藏层的工作是将输入转换为输出层可以使用的东西。

- 输出层将隐藏层激活转换为您希望输出所在的任何比例。

我们大概使用一个图来说明这一切

![一个隐藏层](img/deep_2023-10-30_14-33-26.png)

我们假设X是是任意的一个几何图形，Y是我们最终得到的它是哪个图形的概率或者得分,此时在隐藏层，我们可以发现它就是三种已经确认好的图形特征，隐藏层无非在确认一件事情，你是像我更多一点，还是像它更多一点。如果你的输入是菱形，那么结果Y必然会显示出，菱形得分或者概率更大。

如果你还不太明白，那么我们换成一个简单的数学问题

![一个隐藏层](img/deep_2023-10-30_14-27-06.png)

事实上隐藏层就是一个将输入特征放大的关键位置，为之后的输出提供很重要的依据。

---

### 隐藏层数量

之前的例子中，我们讲解的判断图形是什么形状，如果还需要我们判断图形是什么颜色，则我们就会得到如下图

![两个隐藏层](img/deep_2023-10-30_14-37-19.png)

这样我们就可以判断图形概率后，大概率是什么样的颜色

所以我们在这里可以得出，隐藏层的数量越多，可以将特征匹配得更加细致。

### 隐藏层的神经元（算子）个数

事实上，通过上述给大家所讲例子，很多同学，应该已经明白神经元个数，是与结果有关系的。

但由于深度神经网络一般解决现实中的复杂问题，这个数量并不是一个可以推导的确定的值。

提高隐藏层的神经元或者算子的数量，可以加速分类问题的求解，使其结果快速收敛，控制损失率等等

### 输入和输出如何定义

上述的案例中，所有的输入都只有一个，但现实中，对于需要判定内容的输入，是有很多特征点的。

![全连接](img/deep_2023-10-30_14-55-34.png)

如图所示。

输入：我们需要知道样本提供的特征数量，也就是图中x1,x2,x3. 例如，一个坐标轴给我们提供了只有xy两个值。

输出：我们需要知道经过整个神经网络，我们需要得到得结果是什么，例如，今天下不下雨，石头剪刀布的概率有多大。（交叉熵部分详细讲解）

### 全连接（Dense）定义

在图像分类模型中，Dense是一种常用的神经网络层，也称为全连接层或密集连接层。它的作用是将前一层（通常是卷积层或池化层）的输出进行扁平化并连接到输出层或下一个隐藏层。这样做的目的是将高维的特征表示转换为一维向量，并将这些特征用于最终的分类任务。

Dense层的工作原理如下：

+ 输入扁平化：卷积层和池化层的输出是二维（或更高维）特征图，每个特征图对应一个特征。Dense层将这些特征图转换为一个长向量，以便输入到下一层。

+ 连接所有特征：将扁平化的向量连接到Dense层后，每个神经元都与前一层的所有特征相连，这使得Dense层能够捕捉输入数据中所有特征之间的复杂关系。

+ 学习权重：Dense层中的每个连接都有一个权重，模型通过训练过程中调整这些权重来学习从输入特征到输出类别的映射。这些权重使得模型能够在训练数据上学习到合适的特征表示和分类决策。

+ 非线性激活函数：通常，在Dense层之后会应用一个非线性激活函数（如ReLU），以增加模型的表示能力并引入非线性因素，从而能够解决更加复杂的分类问题。

综上所述，Dense层在图像分类模型中扮演着关键的作用，它负责将图像特征转换为适用于分类任务的向量表示，并通过学习权重来实现准确的分类。

## 链式求导法则

在`反向传播`中，我们会学习到一个很重要的理论，那就是通过不断调整w、b的参数变化，我们可以得到合适的w、b的对应系统：

**w新 = w旧 - 学习率（小固定值） x 斜率**

那么在深度神经网络中，这样的方式该如何构建呢

如下图：

![链式求导法则](img/deep_2023-10-30_16-05-54.png)

$$
z_{11} = w_{11}x_{1} + b \\
z_{21} = w_{12}x_{1} + b \\
z_{12} = w_{21}z_{11} + w_{23}z_{21} + b \\
z_{22} = w_{24}z_{21} + w_{22}z_{11} + b \\
y = w_{31}z_{12} + w_{33}z_{22} + b \\
$$

上述公式带入系数，我们可以得$ z_{12} $到一个获得简易的公式：(此时我们先讨论简单的部分)

$$
z_{12} = w_{21}w_{11}x_{1} + w_{23}w_{12}x_{1} + b
$$

如果以上述公式分别对四个w值求导，比较麻烦，但计算单个连接的求导时，又会很容易。
因此在不考虑高数理论的情况下，我们可以这样认为

$$
z_{11} = w_{11}x_{1} + b \\
z_{21} = w_{12}x_{1} + b \\
z_{12} = w_{21}z_{11} + w_{23}z_{21} + b \\
$$

上述求导可以变化为

$$
\frac{dz_{12}}{dz_{11}}.\frac{dz_{11}}{dw_{11}} \\
\frac{dz_{12}}{dz_{21}}.\frac{dz_{21}}{dw_{12}} \\
\frac{dz_{12}}{dw_{21}} \\
\frac{dz_{12}}{dw_{23}}
$$

其中前两条便是实际开发中需要的链式求导法则
也就是说对于结果，而言可以求得任意层关于对应w的导数关系

```python

```
