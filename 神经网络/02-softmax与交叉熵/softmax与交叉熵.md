# softmax 与交叉熵



[TOC]

## 损失函数

机器学习的逻辑回归中，我们已经知道了`最小二乘法`，`极大似然估计`

对于最小二乘法，实际上就是我们后来知道的`MSELoss`均方差损失，

而极大似然估计法，则是我们在二分类问题中的另一个重要问题`交叉熵`



### 最小二乘法

最小二乘法，实际上是衡量真实结果与预测结果之间差距的一种方式
$$
min \sum_{i=1}^{n} (x_i - y_i)^2
$$

> 思考：为什么需要平方？
>
> 1、绝对值
>
> 2、函数连续性，可导性考虑



### 极大似然估计法

#### 什么是似然值与极大似然值

此处我们以抛硬币的案例说明，我们都知道抛硬币在理论上的概率分布如下：

|        | 正面概率 | 反面概率 |
| ------ | -------- | -------- |
| 概率值 | 0.5      | 0.5      |

在理论中就算抛硬币，抛无论多少次，最终概率结果都会不变。

但是事实上，我们往往会得不到想要的理想结果。我们对硬币抛十次，得到的概率如下：

|        | 正面概率 | 反面概率 |
| ------ | -------- | -------- |
| 概率值 | 0.1      | 0.9      |

也就是反面总共出现了9次，正面总共出现了1次，为了描述这种情况，我们设计了一个概率公式
$$
P(C_1,C_2,C_3,……,C_{10} | \theta) \\
其中 \theta 是概率模型，比如 0.1:0.9 就是我们的概率模型 \\
C_1,C_2,C_3,……,C_{10} 是我们每一次抛硬币的概率 \\
由于抛硬币是连续事件因此对于模型发生的概率 \\
P = 0.1 \times 0.9 \times 0.9 \times ... \times0.9 \\
我们称P的值就是对应概率模型的似然值
$$
当我们存在多个概率模型时

|           | 正面概率 | 反面概率 |
| --------- | -------- | -------- |
| 概率模型1 | 0.1      | 0.9      |
| 概率模型2 | 0.7      | 0.3      |
| 概率模型3 | 0.2      | 0.8      |

通过上述的似然值公式，我们求出似然值最接近理想值的概率模型，这就是`极大似然值`，而该概率模型实际上最接近，我们理想中的概率模型。



#### 极大似然值与神经网络

如果说抛硬币的极大似然由θ概率模型决定，则在前向传播中的预测值，是由w、b来决定的。
$$
P(x_1,x_2,x_3,...,x_n | w,b) \\
= \prod_{i=1}^{n} P(x_i|w,b) \\
其中此处的x_i是指的每种类别的标签 \\
如果y_i = f(w,x) + b,y_i是对应该类别的概率值 \\
 = \prod_{i=1}^{n} P(x_i|y_i) \\
 根据伯努利分布可知 \\
 当 x_i \in \{0,1\} \\
 且 f(x) = p^x(1-p^{1-x}) = \begin{cases} p & ,x=1 \\ 1-p &,x = 0 \end{cases} \\
 = \prod_{i=1}^{n} y_i^{x_i}(1-y_i)^{1-x_i} \\
 引入 \log函数后，保持不变单调性 \\
 = \log(\prod_{i=1}^{n} y_i^{x_i}(1-y_i)^{1-x_i}) \\
 = \sum_{i=1}^{n}\log(y_i^{x_i}(1-y_i)^{1-x_i}) \\
 = \sum_{i=1}^{n}(x_i \log y_i + (1 - x_i) \log(1 - y_i)) \\
此时我们得到的公式便是极大似然值公式
$$


## 交叉熵

最小二乘法、极大似然估计都在定义两个模型的差距，而交叉熵则利用了熵的概念，先将模型换成熵这个值，然后利用这个数值去比较不同模型之间的差异。



### 信息论

信息论中有一个很重要的概念就是信息量。

英雄联盟比赛中的信息论

<img src="./assets/image-20240820141800253.png" alt="image-20240820141800253" style="zoom:50%;" />

参赛队伍有BLG、T1、TL、FNC、GEN、TOP、G2、Fly

1. 对于BLG如果能从1/8比赛取得总冠军，那么我们认为信息量的大小是1/8 - 1

2. 对于BLG如果能从1/8比赛进入到1/2决赛，那么信息量的大小为1/8 - 1/2

如果要横向上述信息量的大小，所以给定了这样的公式
$$
熵 = - \log_2x
$$
因此对于1而言，信息量的大小为：
$$
3 = - \log_2 \frac{1}{8} - (- \log_2 {1})
$$
而对于2而言，信息量的大小为：
$$
2 = - \log_2 \frac{1}{8} - (- \log_2 \frac{1}{2})
$$
因此我们认为1的信息量要比2要大。



但是以上的信息量，是我们一厢情愿，因为实际上我们只有等某些事情发生后，才能得出这些值。

信息量只能衡量一件事情从不确定到确定的难度大小。 而熵的概念就是一个系统从不确定到确定的难度大小。



所以，如果在赛前我们已知BLG能够取胜的概率为1 / 8，则我们可以评估它对系统信息量的贡献（熵）。
$$
熵 = 预测概率 \times 信息量 \\
所以我们可以得到如下公式:\\
-\sum_{i=1}^{n} p_i \cdot \log_2p_i
$$


### 相对熵（KL散度）

相对熵主要用于衡量信息量（模型与模型）的差距
$$
D_{LK}(P||Q) 
P为基准，考虑Q跟它相差多少 \\
= \sum_{i=1}^{m}p_i \cdot (f_q(q_i) - f_p(p_i)) \\
= \sum_{i=1}^{m}p_i \cdot ((-\log_2q_i) - (-log_2p_i)) \\
= \sum_{i=1}^{m}p_i (-\log_2q_i)- \sum_{i=1}^{m}p_i (-log_2p_i) \\
此处：\\
\sum_{i=1}^{m}p_i (-\log_2q_i) 被称为P的交叉熵，记为H(P,Q) \\
而\sum_{i=1}^{m}p_i (-log_2p_i) 实际上就是P的熵 \\

由于吉布斯不等式，已经证明 \\
\sum_{i=1}^{m}p_i (-\log_2q_i)- \sum_{i=1}^{m}p_i (-log_2p_i) \ge 0
$$


### 结论中的交叉熵

$$
由相对熵的证明 要想要两个模型无线接近，则他们的交叉熵必须取到最小值。\\
因此对于H(P,Q) 交叉熵的值对该公式 \\
H(P,Q) \\
= \sum_{i=1}^{m}p_i (-\log_2q_i) \\
由于p_i 是类别概率（理论上的概率），q_i 是预测概率（实际中的概率）\\
当p_i理论上发生时，便有1-p_i绝不会发生，则 我们要确保是对应的概率同时存在 \\
=  -\sum_{i=1}^{m}(p_i \cdot \log_2{q_i} + (1 - p_i) \cdot \log_2(1 - q_i)) \\
此公式也是交叉熵公式也是极大似然公式
$$



## softmax

我们已经学习了大量的激活函数：

+ sigmoid
+ tanh
+ ReLU

..

对于sigmoid以及tanh函数的区间因为在[0,1]因此我们有时可以利用他们做一些简单的在逻辑回归中的二分类问题



而softmax是激活函数，它的公式:

$$
softmax=\frac{e^{x}}{\sum_{i=1}^n e^{x}_{i}}
$$


函数曲线：



通过函数的特征，可知

+ 归一化：Softmax函数可以将一组实值映射为概率分布，即输出的一组值之和为1。这使得其可以用于多分类问题中，并能够体现模型对不同类别的概率预测。

+ 概率输出：由于Softmax函数输出的结果总和为1，且每个输出值都介于0和1之间，所以它能够给出模型对于每个类别的概率预测。

+ 非线性：Softmax函数是一个非线性函数，可以将模型的输出映射到0-1之间，增强了模型的表达能力。

+ 稳定性：Softmax函数对于输入的微小变化具有一定的稳定性，即它不会因为输入的微小变化而产生过大的输出变化。

+ 易于计算：Softmax函数具有简单的计算形式，易于在各种深度学习框架中实现。

+ 输出可解释性强：由于Softmax函数的输出为概率分布，因此其结果具有直观的可解释性，使得模型更容易理解和解释。

### 归一化

由于

$$
\frac{e^{x}_{1}}{e^{x}_{1} + e^{x}_{2} + e^{x}_{3}}+
\frac{e^{x}_{2}}{e^{x}_{1} + e^{x}_{2} + e^{x}_{3}}+
\frac{e^{x}_{3}}{e^{x}_{1} + e^{x}_{2} + e^{x}_{3}}=1
$$

在同一环境下的内容分类问题，大都是“更像谁”，而不是“是谁”，单个概率保证了非负且介于0~1之间

### softmax中的交叉熵

参考：同交叉熵内容

交叉熵公式为

$$
H(p,q) = \sum_{}{p(x) \log q(x)}
$$

其中`p(x)`为真实概率，`q(x)`为预测概率。

交叉熵是一种常用的`信息论`度量方法，可以衡量两个概率分布之间的`差异程度`，在机器学习和深度学习中，交叉熵常被用来作为损失函数，用于衡量模型预测结果与真实标签的差异，从而优化模型的训练过程。交叉熵越小，说明两个概率分布之间越接近。

除了用于分类问题外，`交叉熵`也可以用于`回归问题`。在回归问题中，我们通常会使用`均方误差（MSE）`作为损失函数来衡量预测值与真实值之间的差异。然而，当`预测值是概率分布`时，使用交叉熵作为损失函数会更合适。