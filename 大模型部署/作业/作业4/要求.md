# 要求

- 使用 xinference 启动 llama 模型
- 使用 langchain 创建模型客户端连接 xinference
- 使用 langchain 链，创建一个管线，管线节点如下:
  1. 第一个节点调用大模型
  2. 第二个节点将模型输出结果追加到聊天记录 messages 中，并返回 messages
  3. 第三个节点，将 messages 中的聊天记录重新渲染，SystemMessage 渲染成 'System: <系统消息>'，HumanMessage 渲染成 'User: <用户消息>'，AIMessage 渲染成 'AI: <AI消息>'
  4. 请尽量保证节点三渲染的每条消息仅占一行
- 调用管线

```python
messages = [
  SystemMessage(content="abc"),
  HumanMessage(content="xyz"),
  AIMessage(content="666")
]

'''
System: abc
User: xyz
AI: 666
'''
```
