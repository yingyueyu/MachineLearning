# 创建模型

这里技术路线分为两种:

1. 深度集成: 将模型整体集成到 `langchain` 中
   - 优点: 灵活、功能齐全、可以满足一切工作需求
   - 缺点: 非常复杂，几乎需要实现所有 `langchain` 的抽象方法
2. 集成 API 服务器: 使用 API 服务器充当 `LLM` 推理引擎，使用 `langchain` 调用 `API` 服务器来实现其他功能
   - 优点: 简单、兼容大部分 `langchain` 的代码
   - 缺点: 部分模型的方言，需要特殊处理，也许会出现一些不可预估的意外

此处我们使用较简单的 API Server 的方法

## 启动 API Server

在 `chatglm.cpp` 项目目录下运行命令，启动服务

```shell
set MODEL=./models/chatglm-ggml.bin && uvicorn chatglm_cpp.openai_api:app --host 127.0.0.1 --port 8000
```

这里我们启动了一个遵守 `openai` 规范的 api server

## 封装创建模型流程

使用 `langchain` 提供的 `ChatOpenAI` 对象来访问 api server

```python
from langchain_openai import ChatOpenAI


def get_model():
    # base_url: 服务器的地址
    # api_key: 可以随便设置
    model = ChatOpenAI(model="default-model", base_url='http://127.0.0.1:8000/v1', api_key='abc')
    return model


if __name__ == '__main__':
    model = get_model()
    print(model.invoke('你好'))
```

测试一下

```python
model = get_model()
print(model.invoke('你好'))
```
