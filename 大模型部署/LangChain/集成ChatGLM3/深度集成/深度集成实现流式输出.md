[toc]

# 实现流式输出

实现流式输出，需要对以下内容进行改造:

- `GLM3(LLM核心工具)` 添加 `stream` 方法
- `ChatGLM` 添加 `stream` 方法
- `ChatGLM3Runnable` 添加 `stream` 方法

这些 `stream` 方法都会使用 `yield` 关键字返回一个可迭代的生成器，返回每次生成的 `token`

## `GLM3(LLM核心工具)` 添加 `stream` 方法

在 LLM 类中，我们需要实现 stream 方法，并在其中调用 `chatglm.cpp` 的流式对话 `api`，完整代码如下:

```python
def stream(
        self,
        input,
        config=None,
        *,
        stop=None,
        **kwargs: Any,
) -> Iterator[str]:
    history = kwargs['history']
    if history is None:
        history = []
    if input.strip() != '':
        history.append(ChatMessage(role='user', content=input))
    _kwargs = {**self.generation_kwargs}
    _kwargs['stream'] = True
    chunks = []
    for chunk in self.model.chat(history, **_kwargs):
        content = chunk.content
        chunks.append(chunk)
        yield AIMessageChunk(content=content)
    message = self.model.merge_streaming_messages(chunks)
    history.append(message)
    self.tool_calls = None if len(message.tool_calls) == 0 else message.tool_calls
    if self.tool_calls is not None:
        yield AIMessageChunk(content=message.content)
```

### 重点解析

#### 1. 开启流式输出

`chatglm.cpp` 的流式输出，可以通过关键字参数 `stream` 来设置

```python
# 克隆参数
_kwargs = {**self.generation_kwargs}
# 开启流式输出模式
_kwargs['stream'] = True
# 后续调用，将 _kwargs 作为关键字参数传入即可
self.model.chat(history, **_kwargs)
```

#### 2. 返回流式输出结果

`langchain` 接口中，我们需要返回 `BaseMessageChunk` 的子类，所以调用 `self.model.chat(history, **_kwargs)` 后，我们需要对每次迭代都封装一个 `AIMessageChunk`，如下:

```python
chunks = []
for chunk in self.model.chat(history, **_kwargs):
    content = chunk.content
    chunks.append(chunk)
    # 封装 AIMessageChunk 并返回
    yield AIMessageChunk(content=content)
```

#### 3. 合并输出

```python
# 合并输出
message = self.model.merge_streaming_messages(chunks)
# 加入历史信息
history.append(message)
```

#### 4. 判断是否需要发出调用工具指令

如果使用流式输出，只有合并后的 `message` 中，才会收到调用工具的信息，所以合并消息后，需要判断是否调用工具

```python
self.tool_calls = None if len(message.tool_calls) == 0 else message.tool_calls
# 若要调用工具，此处需要发送一个工具调用的消息
if self.tool_calls is not None:
    yield AIMessageChunk(content=message.content)
```

## `ChatGLM` 添加 `stream` 方法

我们的聊天任务模型也需要添加 `stream` 方法，完整代码如下:

```python
def stream(
        self,
        input,
        config=None,
        *,
        stop=None,
        **kwargs: Any,
) -> Iterator[BaseMessageChunk]:
    for chunk in self.llm.stream(input, history=self.history, **kwargs):
        # 添加是否调用工具的元数据
        chunk.response_metadata['call_tool'] = self.llm.tool_calls is not None
        yield chunk
```

**重点只有一处**，我们需要添加是否调用工具的元数据，该元数据用于 `Runnable` 中调用工具的判断

## `ChatGLM3Runnable` 添加 `stream` 方法

`ChatGLM3Runnable` 是通过 `bind_tools` 后返回的实例，在该对象中，我们需要调用工具，并输出最终结果

```python
def stream(
        self,
        input,
        config=None,
        **kwargs,
) -> Iterator[Output]:
    for chunk in self.chat_model.stream(input, **kwargs):
        if chunk.response_metadata['call_tool']:
            # 调用工具
            self._call_tool()
            for _chunk in self.chat_model.stream(''):
                yield _chunk
        else:
            yield chunk
```

这里我把调用工具的流程封装到了 `_call_tool` 中

```python
def _call_tool(self):
    tool_result = {}
    for tool in self.chat_model.llm.tool_calls:
        tool_call = self.chat_model.tools_func[tool.function.name]
        call_result = eval(tool.function.arguments)
        tool_result[tool.function.name] = call_result
    self.chat_model.history.append(ChatMessage(role=ChatMessage.ROLE_OBSERVATION,
                                                content=f'对应 tool_name 和调用结果如下: \n{str(tool_result)}'))
```

**注意点如下:**

- 调用工具的流程和之前相同
- 工具返回结果后，我们使用 `self.chat_model.stream('')` 替代原来的 `self.chat_model.invoke('')`

## 测试

```python
model_path = r'D:\projects\chatglm.cpp\models\chatglm-ggml.bin'
model = ChatGLM(model_path)
model = model.bind_tools(tools)
result = model.stream('今天重庆(city_code: 500000)天气怎么样？')
for chunk in result:
    print(chunk.content, end='')
```