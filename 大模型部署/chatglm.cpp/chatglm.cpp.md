# chatglm.cpp

chatglm.cpp æ˜¯ github ä¸Šçš„ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæ˜¯å¯¹ chatglm çš„ c++ å®ç°

è¯¥é¡¹ç›®å¯ä»¥é‡åŒ–åŸç”Ÿçš„ chatglmï¼Œä»¥è¾¾åˆ°ç”¨æ›´ä½çš„ç”µè„‘èµ„æºæ¥æ›´é«˜æ•ˆçš„è¿è¡Œå¤§æ¨¡å‹çš„ç›®çš„

å¯ä»¥å°†è¯¥é¡¹ç›®ä½œä¸ºåŸç”Ÿ ChatGLM çš„å¹³æ›¿

[github åœ°å€](https://github.com/li-plus/chatglm.cpp)

chatglm.cpp ä¹Ÿæ˜¯ä¸€ä¸ªå¤§æ¨¡å‹å¼•æ“ï¼Œè¯¥é¡¹ç›®å— llama.cpp çš„å¯å‘è€Œåˆ›å»ºï¼Œä»–ä»¬è¿è¡Œçš„æ¨¡å‹æ ¼å¼ä¸º ggml æ¨¡å‹

> å½“æˆ‘ä½¿ç”¨ ubuntu 22 æ­å»º llama.cpp æ—¶ï¼Œå‘ç°å…¶äºŒè¿›åˆ¶å‘½ä»¤è¡Œå·¥å…·éœ€è¦åœ¨æ›´é«˜ç‰ˆæœ¬çš„ ubuntu ä¸­æ‰èƒ½è¿è¡Œï¼ˆUbuntu 23.10 ä»¥ä¸Šï¼‰

## æ­å»ºç¯å¢ƒ

å…‹éš†é¡¹ç›®

```shell
# æ³¨æ„ï¼šéœ€è¦æ·»åŠ  --recursive
git clone --recursive https://github.com/li-plus/chatglm.cpp.git
```

åˆ›å»º conda ç¯å¢ƒ

```shell
conda create --name chatglm_cpp python=3.10
```

å®‰è£…ä¾èµ–

```shell
pip install -U pip
pip install torch tabulate tqdm transformers accelerate sentencepiece
```

## è½¬æ¢é‡åŒ–æ¨¡å‹

```shell
# -i THUDM/chatglm-6b æ˜¯æœ¬åœ°æ¨¡å‹å‚æ•°è·¯å¾„
python chatglm_cpp/convert.py -i THUDM/chatglm-6b -t q4_0 -o models/chatglm-ggml.bin
```

æœ€åä¼šè¾“å‡ºä¸€ä¸ª GGML æ ¼å¼çš„æ¨¡å‹æ–‡ä»¶ `models/chatglm-ggml.bin`

## æ„å»ºå’Œè¿è¡Œï¼ˆéå¿…è¦ï¼‰

> æ³¨æ„: ä¸ºäº†è¿è¡Œ cmakeï¼Œéœ€è¦æå‰å®‰è£… c++ ç¼–è¯‘ç¯å¢ƒå’Œ cmake
> è¿™é‡Œä½¿ç”¨çš„å¾®è½¯ `vs_BuildTools.exe` å’Œ `cmake-3.30.3-windows-x86_64.zip`


æ„å»ºåä¼šç”Ÿæˆä¸€ä¸ª `main.exe` æ–‡ä»¶ï¼Œç„¶åå°±å¯ä»¥ä½¿ç”¨å‘½ä»¤è¡Œå¾…ç”¨å¤§æ¨¡å‹ç¨‹åº

æ„å»ºå‘½ä»¤å¦‚ä¸‹:

```shell
cmake -B build
cmake --build build -j --config Release
```

æ„å»ºå¥½åè¿è¡Œå‘½ä»¤è¡Œå¼€å§‹å¯¹è¯

```shell
# -p æç¤ºè¯
./build/bin/main -m models/chatglm-ggml.bin -p ä½ å¥½
# ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
# äº’åŠ¨æ¨¡å¼å¯åŠ¨ç¨‹åº
./build/bin/main -m models/chatglm-ggml.bin -i
# æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯
./build/bin/main -h
```

## åŠ è½½é¢„è½¬æ¢çš„ GGML æ¨¡å‹

é¦–å…ˆå®‰è£… `chatglm.cpp` é¡¹ç›®è‡ªèº«

```shell
pip install .
```

å°†å®‰è£…åç”Ÿæˆçš„ `build/lib.win-amd64-cpython-310/chatglm_cpp/_C.cp310-win_amd64.pyd` å¤åˆ¶åˆ° `chatglm_cpp/_C.cp310-win_amd64.pyd`

æ¥ä¸‹æ¥å°±å¯ä»¥åˆ›å»ºè„šæœ¬ï¼Œå¯¼å…¥é‡åŒ–åçš„æ¨¡å‹äº†

åˆ›å»ºè„šæœ¬ `load_ggml_demo.py` å¦‚ä¸‹:

```python
import chatglm_cpp

pipeline = chatglm_cpp.Pipeline("models/chatglm-ggml.bin")
messages = pipeline.chat([chatglm_cpp.ChatMessage(role="user", content="ä½ å¥½")])
print(messages)
# æ‰“å°ç»“æœ: ChatMessage(role="assistant", content="ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM3-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚", tool_calls=[])
```

## LangChain API

æˆ‘ä»¬åœ¨ä½¿ç”¨ LangChain æ—¶ï¼Œéœ€è¦ä¸€ä¸ªæ¨¡å‹æœåŠ¡ç¨‹åºæ¥æä¾›é—®ç­”æœåŠ¡åŠŸèƒ½ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ `chatglm.cpp` æ¥æä¾›é—®ç­”åŠŸèƒ½

å…ˆå®‰è£…æœåŠ¡å™¨ä¾èµ–å¦‚ä¸‹ï¼š

```shell
pip install chatglm-cpp[api]
```

å¯åŠ¨ `LangChain API` æœåŠ¡å™¨

```shell
set MODEL=./models/chatglm-ggml.bin && uvicorn chatglm_cpp.langchain_api:app --host 127.0.0.1 --port 8000
```

æµ‹è¯•æœåŠ¡å™¨æ˜¯å¦æ­£å¸¸è¿è¡Œ

```shell
curl http://127.0.0.1:8000 -H "Content-Type: application/json" -d "{\"prompt\": \"ä½ å¥½\"}"
```

æœåŠ¡æ­å»ºå¥½äº†ï¼Œæ¥ä¸‹æ¥éœ€è¦ä½¿ç”¨ `LangChain` æ¥è°ƒç”¨æœåŠ¡å™¨ï¼Œå…ˆå®‰è£… `LangChain` ä¾èµ–

```shell
pip install -U langchain-community
```

ç¼–å†™è„šæœ¬ `langchain_demo.py` å¦‚ä¸‹

```python
from langchain_community.llms import ChatGLM

llm = ChatGLM(endpoint_url="http://127.0.0.1:8000", max_token=2048, top_p=0.7, temperature=0.95, with_history=False)
print(llm.invoke("ä½ å¥½"))
```

è¿è¡ŒæŸ¥çœ‹ç»“æœ

## OpenAI API

å¯ä»¥ç›´æ¥ç”¨ `chatglm.cpp` æ›¿ä»£ `OpenAI` çš„ API Serverï¼Œä½†æ˜¯å› ä¸ºæ¯•ç«Ÿç”¨çš„ä¸æ˜¯ `ChatGPT` æ¨¡å‹ï¼Œæ‰€ä»¥å¯¹è¯è¿‡ç¨‹ä¸­çš„ä¸€äº›æ•°æ®æ¥å£æ˜¯ä¸èƒ½ç­‰ä»·äº `OpenAI` çš„æ¥å£çš„

```shell
# linux ç³»ç»Ÿå‘½ä»¤
MODEL=./models/chatglm-ggml.bin uvicorn chatglm_cpp.openai_api:app --host 127.0.0.1 --port 8000
# windows ç³»ç»Ÿå‘½ä»¤
set MODEL=./models/chatglm-ggml.bin && uvicorn chatglm_cpp.openai_api:app --host 127.0.0.1 --port 8000
```