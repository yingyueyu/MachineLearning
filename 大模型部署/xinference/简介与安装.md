# 简介与安装

只有 xinference 0.13.2 支持 ggml 格式

更新的版本只支持 gguf 格式

[0.13.2 文档](https://inference.readthedocs.io/en/v0.13.2/getting_started/installation.html#installation-ggml)


[NameError: Field name "schema" shadows a BaseModel attribute; use a different field name with "alias='schema'" 错误参考](https://github.com/xorbitsai/inference/issues/2032)

```shell
pip install openai==1.39.0
```

[官网](https://inference.readthedocs.io/zh-cn/latest/index.html)

Xorbits Inference (Xinference) 是一个开源平台，用于简化各种 AI 模型的运行和集成。借助 Xinference，您可以使用任何开源 LLM、嵌入模型和多模态模型在云端或本地环境中运行推理，并创建强大的 AI 应用。

> **注意:** 该框架目前只支持 llama.cpp 的 ggml 和 gguf 模型，不支持 chatglm.cpp 的 ggml 模型，因为 chatglm.cpp 的 ggml 模型二进制结构和 llama.cpp 的不同，所以 xinference 无法使用 llama.cpp 引擎加载 chatglm.cpp 的 ggml 模型
>
> ==所以这里使用 llama 的 gguf 量化模型来作为基础模型，模型下载在这里（[modelscope](https://www.modelscope.cn/models/QuantFactory/Llama-3.2-3B-GGUF) | [huggingface](https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF)）==

## 对框架的定位

xinference 是一个运行大模型的平台，它的主要责任是运行模型，管理模型，为其他的应用提供模型服务

通常可以不使用 xinference，但是他有以下一些有点:

- 统一的接口: 使用相同的 API 服务器接口，无论你使用什么模型，网络通信的过程都是一样的
- 企业级的模型部署: 支持 GUI 界面，通过简单的操作配置模型并部署模型
- 统一的管理: 集中统一管理所有用到的模型

若你的项目需要多种模型互相配合，则可以考虑使用此框架

## 安装

[安装文档](https://inference.readthedocs.io/zh-cn/latest/getting_started/installation.html#llama-cpp-backend)

首先安装 `xinference` 本体

```shell
pip install xinference
```

然后安装 `llama.cpp` 的 `python` 引擎

```shell
pip install llama-cpp-python
```

> 注意: 安装 `llama-cpp-python` 时需要本地 c 和 c++ 编译环境，若没有则可以在项目仓库中的 [release](https://github.com/abetlen/llama-cpp-python/releases) 中直接下载
> 最新版本 v0.3.1 依然存在系统 bug，在我们的 ubuntu22 系统上依然无法运行

## 安装参考

版本

```
ubuntu 24
python 3.10
xinference 0.16.1
llama_cpp_python 0.3.1
```

不同的 .gguf .ggml 对应不同版本的 llama_cpp_python

所以通常最新的模型需要对应最新的引擎

运行时出错: 

```
RuntimeError: Failed to load shared library '/home/master/miniconda3/envs/llm/lib/python3.10/site-packages/llama_cpp/lib/libllama.so': libc.musl-x86_64.so.1: cannot open shared object file: No such file or directory
```

这是缺少 `musl` 库，我们需要执行以下命令安装

```shell
sudo apt-get install musl-dev
```

然后

```shell
sudo ln -s /usr/lib/x86_64-linux-musl/libc.so /lib/libc.musl-x86_64.so.1
```

将 `/usr/lib/x86_64-linux-musl/libc.so` 链接到 `/lib/libc.musl-x86_64.so.1`

[参考](https://github.com/alexander-akhmetov/python-telegram/issues/3)