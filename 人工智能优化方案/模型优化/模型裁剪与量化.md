# 模型裁剪与量化

## 模型裁剪

### 什么是模型裁剪？

模型裁剪（Model Pruning）是一种减少模型参数数量的方法，用于减小模型的尺寸、提高推理速度以及减少计算资源和内存使用。模型裁剪主要通过识别和移除不重要的权重（即那些对模型预测影响较小的权重）来实现。

### 模型裁剪的原理

模型裁剪的基本思想是通过以下步骤来减小模型规模：

- 评估权重重要性：首先，计算每个权重的重要性。这可以通过各种方法来实现，例如基于权重的绝对值大小、基于梯度的敏感性分析等。

- 移除不重要的权重：根据评估结果，将那些被认为不重要的权重设为零或直接移除。

- 微调模型：裁剪后的模型通常需要重新训练（或微调）一段时间，以恢复部分因裁剪而损失的性能。

重复上述步骤：这个过程可以多次重复，逐步精细化裁剪。

### 例子

模型裁剪不会修改模型的结构和参数，而是在模型参数上添加了一个掩码，用于参数优化时，不修改掩码中指定的权重值，从而达到只修改裁剪后的部分参数的目的

```python
import torch
import torch.nn as nn
import torch.optim as optim
# 导入 pytorch 模型裁剪的工具
import torch.nn.utils.prune as prune

# 定义一个简单的线性模型
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(10, 10)
    
    def forward(self, x):
        return self.fc(x)

model = SimpleModel()

# 对模型进行L1非结构化裁剪，裁剪50%的权重
prune.l1_unstructured(model.fc, name='weight', amount=0.5)

# 打印裁剪后的权重掩码
print("Weight mask after pruning:")
print(model.fc.weight_mask)

# 创建优化器和损失函数
optimizer = optim.SGD(model.parameters(), lr=0.01)
criterion = nn.MSELoss()

# 生成一些随机数据进行训练
x = torch.randn(1, 10)
y = torch.randn(1, 10)

# 训练模型
for epoch in range(10):
    optimizer.zero_grad()
    output = model(x)
    loss = criterion(output, y)
    loss.backward()
    optimizer.step()
    
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")

# 打印训练后的权重
print("Weights after training:")
print(model.fc.weight)
```

### API 汇总

在 PyTorch 中，torch.nn.utils.prune 模块提供了多种用于模型裁剪的 API。这些 API 可以实现各种裁剪方法，包括非结构化裁剪、结构化裁剪等。以下是一些常用的裁剪函数及其参数和返回值的解释：

#### 1. torch.nn.utils.prune.l1_unstructured

**描述**: 使用L1范数进行非结构化裁剪，将权重的绝对值最小的部分裁剪为零。

**参数**:
- module: 要裁剪的神经网络层（例如torch.nn.Linear）。
- name: 要裁剪的参数的名称（例如weight或bias）。
- amount: 要裁剪的权重比例（0到1之间的浮点数）或裁剪的权重数量（整数）。

```python
# amount=0.5 代表 50% 的权重参数被裁剪
# 当 amount 为整数时，代表具体裁剪多少个参数
prune.l1_unstructured(module, name='weight', amount=0.5)
```

#### 2. torch.nn.utils.prune.random_unstructured

**描述**: 随机选择部分权重进行非结构化裁剪。

**参数**:
- module: 要裁剪的神经网络层。
- name: 要裁剪的参数的名称。
- amount: 要裁剪的权重比例或数量。

```python
prune.random_unstructured(module, name='weight', amount=0.5)
```

#### 3. torch.nn.utils.prune.ln_structured

**描述**: 使用L_n范数进行结构化裁剪，裁剪整个神经元或卷积核。

关于 L_n 范数:

- L1: 绝对值的和
- L2: 平方和的平方根
- L3: 立方和的立方根
- L4: 四次方和的四次方根
- Ln: 以此类推

**参数**:
- module: 要裁剪的神经网络层。
- name: 要裁剪的参数的名称。
- amount: 要裁剪的权重比例或数量。
- n: 用于计算L_n范数的指数。
- dim: 指定沿哪个维度进行裁剪（例如，对于卷积层，通常选择输出通道维度）。

```python
prune.ln_structured(module, name='weight', amount=0.5, n=1, dim=0)
```

#### 4. torch.nn.utils.prune.remove

**描述**: 永久地移除裁剪后的掩码和重参数，使裁剪成为模型的一部分。

torch.nn.utils.prune.remove 是 PyTorch 中用于永久移除已应用的裁剪掩码（mask）和重参数（reparameterization）的 API。它将裁剪操作应用的结果固化到模型的参数中，使裁剪后的权重成为模型的正式参数，不再依赖于裁剪模块和掩码。

**重参数化（reparameterization）** 是指将模型的参数分解为多个组件，以便在训练和推理过程中灵活地应用不同的变换或约束。具体到 PyTorch 的裁剪工具中，重参数化是一种技术，通过引入掩码（mask）来动态控制哪些参数在实际计算中被使用，从而实现裁剪效果。

**参数**:
- module: 要移除裁剪的神经网络层。
- name: 要移除裁剪的参数的名称。

```python
prune.remove(module, 'weight')
```

#### 5. torch.nn.utils.prune.custom_from_mask

**描述**: 使用自定义掩码进行裁剪。

**参数**:
- module: 要裁剪的神经网络层。
- name: 要裁剪的参数的名称。
- mask: 一个与权重相同维度的布尔张量，指定哪些权重被裁剪（为0）。

```python
mask = torch.rand(module.weight.shape) > 0.5
prune.custom_from_mask(module, name='weight', mask=mask)
```

#### 6. torch.nn.utils.prune.global_unstructured

**描述**: 在整个模型范围内使用指定的裁剪方法进行非结构化裁剪。

**参数**:
- parameters: 包含要裁剪的参数的列表，每个元素是一个元组（模块, 参数名称）。
- pruning_method: 要使用的裁剪方法，例如prune.L1Unstructured。
- amount: 要裁剪的权重比例或数量。

```python
prune.global_unstructured(
    [(module1, 'weight'), (module2, 'weight')],
    pruning_method=prune.L1Unstructured,
    amount=0.5
)
```

注意: 全局裁剪作为了解，官方并不推荐

## 模型量化

### 什么是模型量化？

用降低模型参数精度的方式，来减少模型大小，提升模型运算效率，这种方法叫做模型量化

量化分为，推理时量化和训练时量化，其中训练时量化不太常用

#### 推理时量化

步骤:

- 准备模型：首先，你需要有一个预训练的模型。
- 准备数据：量化过程需要一些输入数据来校准模型。
- 模型融合：在量化之前，可能需要将模型中的某些操作融合在一起，以优化性能。
- 选择量化配置：定义量化配置，如量化类型（动态或静态）和量化精度。
- 量化模型：使用定义的配置对模型进行量化。
- 评估量化模型：检查量化后的模型性能。

```python
import torch
import torch.nn as nn
import torch.quantization

# 示例模型
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(10, 5)

    def forward(self, x):
        x = self.fc(x)
        return x

# 初始化模型
model = SimpleModel()
model.eval()

# 准备校准数据
calibration_data = torch.randn(10, 10)

# 模型融合
model = torch.quantization.fuse_modules(model, ['linear'])

# 定义量化配置
quantization_config = torch.quantization.get_default_qconfig('fbgemm')

# 为模型设置量化配置
model.qconfig = quantization_config

# 准备模型进行量化
torch.quantization.prepare(model, inplace=True)

# 校准模型
model(calibration_data)

# 对模型进行量化
torch.quantization.convert(model, inplace=True)

# 检查量化模型
print(model)
```

##### 关于模型层的融合

假设模型为:

```python
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.linear = nn.Linear(10, 5)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.linear(x)
        x = self.relu(x)
        return x
```

融合 linear 和 relu

```python
fused_model = torch.quantization.fuse_modules(model, ['linear', 'relu'])
```

融合后的模型为:

```
SimpleModel(
  (linear): LinearReLU(
    (0): Linear(in_features=10, out_features=5, bias=True)
    (1): ReLU()
  )
  (relu): Identity()
)
```

注意到 relu 变成了 Identity，说明融合后 relu 不会做任何操作

融合也能传入序列进行融合，例如:

```python
import torch
import torch.nn as nn
import torch.quantization

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv_bn_relu = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        x = self.conv_bn_relu(x)
        return x

# 初始化模型
model = SimpleCNN()
model.eval()

# 融合卷积层、批量归一化和ReLU层
model = torch.quantization.fuse_modules(model.conv_bn_relu, ['0', '1', '2'])

# 检查融合后的模型
print(model)
```

##### 关于量化配置

量化配置可以选择预设配置，也能选择自定义配置，大部分情况预设就足够了。

```python
# 定义量化配置
# 预设:
# fbgemm：针对x86 CPU的量化后端。
# qnnpack：针对ARM CPU的量化后端。
# float16：用于浮点16位量化。
quantization_config = torch.quantization.get_default_qconfig('fbgemm')
```

其中 fbgemm 和 qnnpack 量化，将整数量化为 8bit，浮点数量化为 16bit

float16 将把所有数字量化为 16bit

##### 自定义量化配置

如果您需要自定义量化配置，可以创建一个QConfig对象，并为其指定以下组件：

- weight_observer：用于观察和记录权重数据的量化参数。
- act_observer：用于观察和记录激活数据（即中间层输出）的量化参数。
- weight_fake_quant：用于模拟权重量化的伪量化器。
- act_fake_quant：用于模拟激活量化的伪量化器。

以下是一个自定义量化配置的示例：

```python
from torch.quantization import QuantStub, DeQuantStub, QConfig

# 自定义量化配置
my_qconfig = QConfig(
    activation=MinMaxObserver.with_args(dtype=torch.quint8),
    weight=PerChannelMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_channel_symmetric)
)
```

#### 量化感知训练（Quantization-Aware Training, QAT）

量化感知训练（Quantization-Aware Training, QAT）是一种在训练过程中模拟量化效应的技术，旨在生成在量化环境下表现良好的权重。QAT允许模型在训练过程中调整权重，以减少量化引入的误差，从而在实际量化后仍能保持良好的性能。

**QAT的关键特点**
- 伪量化：在QAT中，模型在训练过程中会经历伪量化，即模拟量化操作而不实际改变模型的权重和激活值。
- 量化参数调整：通过在训练过程中调整量化参数（如缩放因子和零点），可以优化模型在量化环境下的性能。
- 性能评估：QAT过程中会定期评估模型的性能，以确保量化不会显著降低模型的表现。
- 微调：在某些情况下，量化后的模型可能需要进行微调，以优化其在量化环境下的性能。

**为什么要进行量化感知训练呢？**

为了提前让模型适应量化环境，并在模型量化转换后，能得到一个较为稳定，准确率较好的模型


**QAT的步骤**
- 模型准备：首先，定义您的模型并初始化。
- 量化配置：选择或创建一个量化配置，它定义了量化的类型（动态或静态）、数据类型（如int8或float16）、量化策略等。
- 模型融合：在某些情况下，可能需要将模型中的某些操作融合在一起，以优化性能。
- 准备模型进行量化：使用torch.quantization.prepare方法准备模型进行量化。这将设置模型以模拟量化操作。
- 训练模型：使用常规的训练循环进行训练，但模型会在量化模拟的环境下进行训练。
- 转换模型为量化模型：训练完成后，使用torch.quantization.convert方法将模型转换为真正的量化模型。

示例:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.quantization

# 定义模型
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.linear = nn.Linear(10, 5)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.linear(x)
        x = self.relu(x)
        return x

# 初始化模型
model = SimpleModel()

# 选择量化配置
qat_config = torch.quantization.get_default_qat_qconfig('fbgemm')

# 设置量化配置
model.qconfig = qat_config

# 准备模型进行量化感知训练
torch.quantization.prepare_qat(model, inplace=True)

# 定义损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 训练模型
for epoch in range(10):
    for batch_idx, (data, target) in enumerate(train_loader):
        # 前向传播
        output = model(data)
        loss = criterion(output, target)
        
        # 反向传播和优化
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

# 转换模型为量化模型
torch.quantization.convert(model, inplace=True)
```

在上述代码中，训练循环内已经是在伪量化环境下进行训练了