# 权重初始化

在 PyTorch 中，有多种方法可以初始化神经网络的权重。不同的初始化方法适用于不同的情况，根据你的具体需求和网络结构选择合适的初始化方法可以帮助你提高模型的训练效率和性能。

初始化权重 API 都是就地操作，调用后会直接改变权重值

以下是一些常见的权重初始化方法及其适用情况：

## 1. 随机初始化

### a. torch.nn.init.normal_

**描述**：从正态分布中生成随机数来初始化权重。

```python
import torch.nn as nn
import torch.nn.init as init

layer = nn.Linear(20, 30)
# mean: 均值
# std: 标准差，衡量数字波动幅度
init.normal_(layer.weight, mean=0.0, std=1.0)
```

**适用情况**：适用于需要将权重初始化为均值为 0、标准差为 1 的正态分布的情况。

### b. torch.nn.init.uniform_

**描述**：从均匀分布中生成随机数来初始化权重。

```python
# 区间 [a, b) 内的均匀分布中随机采样的值
# 默认为 [0, 1)
init.uniform_(layer.weight, a=-0.1, b=0.1)
```

**适用情况**：适用于需要将权重初始化为均匀分布的情况。

## 2. Xavier 初始化（Glorot 初始化）

Xavier Glorot 是人名，初始化方法以他的名字命名

### a. torch.nn.init.xavier_normal_

**描述**：从均匀分布中生成随机数来初始化权重，均值为 0，方差为 2 / (fan_in + fan_out)。

- fan_in: 输入神经元个数，对于卷积，就是输入通道数乘以卷积核宽高
- fan_out: 输出神经元个数，对于卷积，就是输出通道数乘以卷积核宽高

```python
init.xavier_normal_(layer.weight)
```

**适用情况**：适用于激活函数为 sigmoid 或 tanh 的网络。

### b. torch.nn.init.xavier_uniform_

**描述**：从均匀分布中生成随机数来初始化权重，方差为 1 / sqrt(fan_in)。

```python
init.xavier_uniform_(layer.weight)
```

**适用情况**：适用于激活函数为 sigmoid 或 tanh 的网络。

## 3. Kaiming 初始化（He 初始化）

何凯明初始化方法

Xavier 初始化（也称为 Glorot 初始化）和 Kaiming 初始化（也称为 He 初始化）都是权重初始化方法，它们的目的是通过适当的初始化来**避免梯度消失或梯度爆炸**问题，从而加速神经网络的训练。

### a. torch.nn.init.kaiming_normal_

**描述**：从正态分布中生成随机数来初始化权重，均值为 0，方差为 2 / fan_in。

```python
# tensor：需要初始化的张量。
# a：负斜率，默认为 0。仅在 nonlinearity 为 leaky_relu 时有效。
# mode：初始化模式，'fan_in' 或 'fan_out'。'fan_in' 模式会保持前向传播过程中输入方差的一致性，'fan_out' 模式会保持反向传播过程中梯度方差的一致性。
# nonlinearity：非线性激活函数的名称，默认为 leaky_relu。常见的选项有 'relu' 和 'leaky_relu'。
init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')
```

**适用情况**：适用于激活函数为 ReLU 或其变体的网络。

### b. torch.nn.init.kaiming_uniform_

**描述**：从均匀分布中生成随机数来初始化权重，方差为 1 / sqrt(fan_in)。

```python
init.kaiming_uniform_(layer.weight, mode='fan_in', nonlinearity='relu')
```

**适用情况**：适用于激活函数为 ReLU 或其变体的网络。

## 4. 常数初始化

### a. torch.nn.init.constant_

**描述**：将权重初始化为常数值。

```python
init.constant_(layer.weight, 0.1)
```

**适用情况**：适用于需要将权重初始化为某个常数值的情况。

## 5. 其他初始化方法

### a. torch.nn.init.eye_

**描述**：将权重初始化为单位矩阵。

- 单位矩阵: 任何矩阵乘以单位矩阵等于他自身。单位矩阵必须是二维的，且行列相等；除了对角线是 1，其余都是 0；例如: 3x3 的单位矩阵是:
  $$
  I = \begin{pmatrix}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1
  \end{pmatrix}
  $$

```python
init.eye_(layer.weight)
```

**适用情况**：适用于需要将权重初始化为单位矩阵的情况，多用于 RNN 的权重初始化。

### b. torch.nn.init.orthogonal_

**描述**：将权重初始化为正交矩阵。

什么是正交矩阵？
正交矩阵是一个方阵，其行向量（或列向量）都是单位向量且彼此正交（即内积为零）。正交矩阵 $Q$ 满足以下条件:

$$
Q^TQ = I
$$

这里 $I$ 是单位矩阵

```python
init.orthogonal_(layer.weight)
```

**适用情况**：适用于需要将权重初始化为正交矩阵的情况，可以在训练深度神经网络时保持较大的激活值方差。常见于初始化 RNN 中的权重。由于 RNN 涉及多次时间步长的权重矩阵乘法，正交矩阵可以帮助保持梯度的稳定性，减轻梯度消失和梯度爆炸问题。

### c. torch.nn.init.sparse_

**描述**：将权重初始化为稀疏矩阵。

**什么是稀疏矩阵**？

稀疏矩阵是包含大量零元素的矩阵。与密集矩阵（dense matrix）不同，稀疏矩阵主要用于高效地存储和计算具有少量非零元素的大规模数据。

```python
init.sparse_(layer.weight, sparsity=0.1)
```

**适用情况**：适用于需要将权重初始化为稀疏矩阵的情况，多用于稀疏编码的网络。torch.nn.init.sparse_ 用于将一个张量初始化为稀疏矩阵。该函数会随机选取一些元素并将它们设置为非零值，其余元素设置为零。稀疏矩阵的主要参数包括非零元素的比例（sparsity）和非零元素的标准差（std）。

**何时使用稀疏初始化？**

稀疏初始化在以下情况中有用：

- 特定的深度学习模型：在某些神经网络（例如自编码器）中，稀疏性可以帮助模型学习到更加有意义的特征。
- 大规模神经网络：在处理大规模神经网络时，稀疏矩阵可以减少计算和存储成本。
- 自然语言处理：在词嵌入和语言模型中，稀疏性可以帮助捕捉更加稀有的词语或特征。