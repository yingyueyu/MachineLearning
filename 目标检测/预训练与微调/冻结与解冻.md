# 冻结与解冻



## 1、引言

迁移学习在计算机视觉领域中是一种很流行的方法，因为它可以建立精确的模型，耗时更短。利用迁移学习，不是从零开始学习，而是从之前解决各种问题时学到的模式开始。这样，我们就可以利用以前的学习成果，避免从零开始。



## 2、使用预训练权重

在计算机视觉领域中，迁移学习通常是通过使用预训练模型来表示的。**预训练模型是在大型基准数据集上训练的模型，用于解决相似的问题**。由于训练这种模型的计算成本较高，因此，导入已发布的成果并使用相应的模型是比较常见的做法。例如，在目标检测任务中，首先要利用主干神经网络进行特征提取，这里使用的backbone一般就是VGG、ResNet等神经网络，因此在训练一个目标检测模型时，可以使用这些神经网络的预训练权重来将backbone的参数初始化，这样在一开始就能提取到比较有效的特征。

可能大家会有疑问，预训练权重是针对他们数据集训练得到的，如果是训练自己的数据集还能用吗？**预训练权重对于不同的数据集是通用的，因为特征是通用的。一般来讲，从0开始训练效果会很差，因为权值太过随机，特征提取效果不明显**。对于目标检测模型来说，一般不从0开始训练，至少会使用主干部分的权值，虽然有些论文提到了可以不用预训练，但这主要是因为他们的数据集比较大而且他们的调参能力很强。如果从0开始训练，网络在前几个epoch的Loss可能会非常大，并且多次训练得到的训练结果可能相差很大，因为权重初始化太过随机。

**PyTorch提供了state_dict()和load_state_dict()两个方法用来保存和加载模型参数，前者将模型参数保存为字典形式，后者将字典形式的模型参数载入到模型当中**。



## 3、冻结训练

冻结训练其实也是迁移学习的思想，在目标检测任务中用得十分广泛。因为目标检测模型里，主干特征提取部分所提取到的特征是通用的，**把backbone冻结起来训练可以加快训练效率，也可以防止权值被破坏**。在冻结阶段，模型的主干被冻结了，特征提取网络不发生改变，占用的显存较小，仅对网络进行微调。在解冻阶段，模型的主干不被冻结了，特征提取网络会发生改变，占用的显存较大，网络所有的参数都会发生改变。 





## 4、关于预训练和微调的总结

+ 什么是预训练
+ 什么是微调

> 神经网络越深，就需要更多的样本进行训练，否则就很容易出现过拟合现象
>
> 采用预训练+微调也不是绝对有效，数据集的相关性很重要。



## 5、关于冻结方式

+ 针对需要冻结的层执行`requires_grad=False`
+ 在优化器中，仅针对需要更新参数的层进行设置



> 实际开发中，最优做法就是优化器`requires_grad=True`的参数，这样占用的内存会小一点，效率也会更高。



**最优写法**

将不更新的参数的`requires_grad`设置为`False`，同时不将该参数传入`optimizer`

1. 将不更新的参数的`requires_grad`设置为`False`

```
# 冻结fc1层的参数 for name, param in model.named_parameters(): if "fc1" in name: param.requires_grad = False
```

1. 不将不更新的模型参数传入`optimizer`

```
# 定义一个fliter，只传入requires_grad=True的模型参数 optimizer = optim.SGD(filter(lambda p : p.requires_grad, model.parameters()), lr=1e-2)
```



最优写法能够节省显存和提升速度：

1. 节省显存：不将不更新的参数传入`optimizer`
2. 提升速度：将不更新的参数的`requires_grad`设置为`False`，节省了计算这部分参数梯度的时间
