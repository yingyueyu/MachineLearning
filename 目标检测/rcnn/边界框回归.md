# 边界框回归



## 一、概述

相比传统的[图像分类](https://zhida.zhihu.com/search?content_id=105152742&content_type=Article&match_order=1&q=图像分类&zhida_source=entity)，**目标检测不仅要实现目标的分类，而且还要解决目标的定位问题，即获取目标在原始图像中的位置信息**。在不管是最初版本的RCNN，还之后的改进版本——Fast RCNN和Faster RCNN都需要利用边界框回归来预测物体的目标检测框。因此掌握边界框回归（Bounding-Box Regression）是极其重要的，这是熟练使用RCNN系列模型的关键一步，也是代码实现中比较重要的一个模块。



以下是对边界框回归的说明
$$
输入到边界框回归的数据集为 \\
{(P^i,G^i)}_{i=1,...,N} \\
其中P^i = (P_x^i,P_y^i,P_w^i,P_h^i),G^i = (G_x^i,G_y^i,G_w^i,G_h^i) \\
P^i 代表第i个带预测的候选目标监测框即region-proposal， \\
G^i 是i个真实目标监测框即ground-turth。\\
对于RCNN和Fast-RCNN，P^i是利用选择性搜索算法进行获取\\
Faster-RCNN中，P^i 是利用PRN（Region Proposal Network，区域生成网络）获取。\\
边界框回归要做的就是利用某种映射关系，使得候选框目标框（region proposal）\\
无线接近于真实目标框（ground-truth），即：\\
候选P = (P_x,P_y,P_w,P_h)，寻找一个映射f，使得 \\
f(P_x,P_y,P_w,P_h) = (\hat G_x,\hat G_y,\hat G_w,\hat G_h) \approx ( G_x, G_y, G_w, G_h)\\
$$


## 二、边界框回归细节



RCNN论文里指出。边界框回归是利用平移变换和尺度变换来实现映射。

平移变换公式：
$$
\hat G_x = P_w d_x(P) + P_x\\
\hat G_y = P_h d_y(P) + P_y
$$
尺度变换公式:
$$
\hat G_w = P_w \exp(d_w(P)) \\
\hat G_h = P_h \exp(d_h(P))
$$
在边界框回归中，我们利用了线性回归在RCNN论文代表这AlexNet第5个池化层得到的特征即将送入全连接层的输入特征的线性函数。

我们将这个线性函数记为
$$
\phi_5(P) \\
则 d_*(P) = w^T_* \phi_5(P) \\
因此我们可以使用最小二乘法或者梯度下降法进行求解，RCNN论文给出的表达式为：\\
w_* = \arg\min_{\hat w_*} \sum_{i=1}^N(t^i_* - w^T_* \phi_5(P^i))^2 + \lambda||\hat w_*||^2\\
其中:\\
t_x = \frac{G_x - P_x}{P_w} \\
t_y = \frac{G_y - P_y}{P_h} \\
t_w = \log \frac{G_w}{P_w} \\
t_h = \log \frac{G_h}{P_h}
$$
上述模型就是一个Ridge回归模型。在RCNN中，边界框回归要设计4个不同的Ridge回归模型，分别求解
$$
w_x,w_y,w_w,w_h
$$


## 三、一些疑问

### 1、为什么使用相对坐标差

等比例缩放问题(中国的地图无论多大，但国土比例是不变的)

### 2、为什么宽高比要取对数

横量目标与预测结果的差距，需要差距越小越好，又要防止负数出现。

### 3、为什么IoU较大时边界框回归可视为线性变换

数学中有关等价无穷小的概念：
$$
\lim_{x \rightarrow 0} \frac{\log(1 + x)}{x} = 1
$$
则
$$
t_w = \log \frac{G_w}{P_w} = \log \frac{G_w - P_w + P_w}{P_w} = \log (1 + \frac{G_w - P_w}{P_w})  \\
t_h = \log \frac{G_h}{P_h} = \log \frac{G_h - P_h + P_h}{P_h} = \log (1 + \frac{G_h - P_h}{P_h}) \\
如果 G_w \approx P_w 和 G_h \approx P_h 时 \\
候选目标狂和真实目标框非常接近，即IoU值比较大 \\
按照RCNN论文的说法，IoU大于0.6时，边界框回归可视为线型变换。
$$
