# 模型评估

## 混淆矩阵与ROC、AUC

模型评估中会用到混淆矩阵相关内容，这里回顾以下 《[ROC曲线](../../../机器学习/机器学习算法/支持向量机/支持向量机理论学习/ROC曲线.md#混淆矩阵-confusion-matrix)》

ROC、AUC 也是衡量模型的方法

## 其他常用衡量标准

### 准确率（Accuracy）

也就是模型预测的正确率，使用预测正确的数量除以所有数据的总量，例如: 100 张图片中，预测正确 80 张，则

$$
Accuracy = \frac{80}{100} = 0.8
$$

### 损失（Loss）

损失也称为**代价（cost）**

### 精确率（Precision）

衡量正例中有多少是真正例

公式为:

$$
Precision = \frac{TP}{TP+FP}
$$

这里举个例子，假设模型预测了 100 个 3，其中 80 个是正确的 3，20 个是其他数字，则精确率为:

$$
Precision = \frac{80}{80 + 20} = 0.8
$$

这个标准是一种衡量二元分类问题时，模型的能力标准，当问题是多分类时，可以使用**宏平均**或**加权平均**的方式将多个类别的准确率一起考虑得到结果。

例如:

假设有 3 个类别，类别 1 的精确率为 0.9，类别 2 的精确率为 0.6，类别 3 的精确率为 0.7。样本数量如下：类别 1 有 50 个样本，类别 2 有 30 个样本，类别 3 有 20 个样本。

- **宏平均**：$\text{Precision}_{\text{macro}} = \frac{0.9 + 0.6 + 0.7}{3} = 0.733$
- **加权平均**：$\text{Precision}_{\text{weighted}} = \frac{(0.9 \times 50) + (0.6 \times 30) + (0.7 \times 20)}{50 + 30 + 20} = 0.77$

### 召回率（Recall）

召回率也叫**查全率**，检查所有正例样本中预测为真的概率

公式为:

$$
Recall = \frac{TP}{TP+FN}
$$

**召回率也能和精确率一样进行宏平均和加权平均**

### F1 Score（F1分数）

F1分数是一个综合考虑精确率和召回率的指标，是他们的调和平均数，公式如下：

$$
F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}
$$

### Top-k精度

在计算准确率（Accuracy）时，我们衡量模型是否预测正确，通常是看预测结果中概率最高的结果是否等于正确结果，这种标准称为 **top-1**

例如:

假设模型预测交通工具分类，`0汽车`、`1飞机`、`2轮船`，给模型一个`1飞机`的图片，模型预测结果是 `[0.5, 0.3, 0.2]`，那么按照 top-1 的标准来看，最大值索引为 0，0是汽车，则模型预测失败

那么假设我们放宽标准，模型预测结果的最大概率的**2个类别**中，只要包含`1飞机`就认为模型预测成功，那么此例模型预测概率最大的索引为 `[0, 1]`，包含了`1飞机`，则模型预测成功，这种标准称为 **top-2**

那么 **top-k** 指的就是最大的 **k** 个概率中是否包含正确结果，包含就认为预测成功

假设刚才例子的样本有 100 个，使用 top-2 标准预测成功的有 80 个，则 top-2 准确率为：

$$
\text{top-2} = \frac{80}{100} = 0.8
$$

常见的 top-k 标准有: top-1, top-5, top-10