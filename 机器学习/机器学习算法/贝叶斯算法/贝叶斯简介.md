# 贝叶斯公式

贝叶斯公式是概率论和统计学中的基本公式，用于描述事件发生概率的条件关系。贝叶斯公式的基本形式如下：

\[ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} \]

其中每一项的含义如下：

1. **\( P(A|B) \)（后验概率）**：
   - 这是在已知事件 \( B \) 发生的情况下，事件 \( A \) 发生的概率。换句话说，这是根据新的证据（即事件 \( B \)）更新后的事件 \( A \) 的概率。

2. **\( P(B|A) \)（似然函数）**：
   - 这是在假设事件 \( A \) 发生的情况下，事件 \( B \) 发生的概率。它表示如果 \( A \) 发生了，那么 \( B \) 发生的可能性有多大。

3. **\( P(A) \)（先验概率）**：
   - 这是在没有考虑事件 \( B \) 的情况下，事件 \( A \) 发生的概率。它代表了我们在获得新的证据之前对事件 \( A \) 发生的初始估计。

4. **\( P(B) \)（边际概率或证据）**：
   - 这是事件 \( B \) 发生的总概率。这是通过考虑所有可能导致 \( B \) 发生的情况计算出来的。它可以看作是一个归一化常数，使得 \( P(A|B) \) 的总和为 1。
   - 当 B 不是一个独立事件时，需要根据公式计算得到 $P(B)$，公式如下:
     $$
     P(B) = \overset{n}{\underset{i=1}\sum} P(B_i|A)P(A)
     $$

## 贝叶斯公式的应用示例

假设你有一个诊断测试，用于检测某种疾病。定义以下事件：
- \( A \)：一个人患有这种疾病。
- \( B \)：这个人的测试结果为阳性。

### 先验概率 \( P(A) \)
这是一个人患有这种疾病的概率，比如 \( P(A) = 0.01 \)（即1%）。

### 似然函数 \( P(B|A) \)

也可以称为**似然值**

这是如果一个人患有这种疾病，测试结果为阳性的概率，比如 \( P(B|A) = 0.99 \)（即测试的敏感性为99%）。

### 边际概率 \( P(B) \)
这是任何一个人测试结果为阳性的总概率。这需要考虑两种情况：一个人患有疾病且测试结果为阳性，或者一个人不患有疾病但测试结果也为阳性（即假阳性）。假设测试的假阳性率为5%，那么：

\[ P(B) = P(B|A)P(A) + P(B|\neg A)P(\neg A) \]
\[ = 0.99 \cdot 0.01 + 0.05 \cdot 0.99 \]
\[ = 0.0099 + 0.0495 = 0.0594 \]

### 后验概率 \( P(A|B) \)
这是在测试结果为阳性的情况下，一个人患有这种疾病的概率：

\[ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} \]
\[ = \frac{0.99 \cdot 0.01}{0.0594} \]
\[ = \frac{0.0099}{0.0594} \approx 0.166 \]

因此，在测试结果为阳性的情况下，一个人实际患有这种疾病的概率约为16.6%。

==可以看到贝叶斯算法主要用来计算满足某种条件下的**条件概率**==

## 递归贝叶斯更新

==贝叶斯的牛逼之处在于，我们**可以假设先验概率**==

在贝叶斯推理中，可以将前一轮计算得到的后验概率作为下一轮的先验概率。这一过程称为“递归贝叶斯更新”或“贝叶斯序贯更新”，用于随着新证据的不断加入，逐步更新对事件发生概率的估计。

具体步骤如下：

1. **初始先验概率 \( P(A) \)**：
   - 这是你在没有任何证据时对事件 \( A \) 的初始估计。

2. **收集第一轮证据 \( B_1 \)**：
   - 计算第一轮证据 \( B_1 \) 下的后验概率 \( P(A|B_1) \)：
   \[ P(A|B_1) = \frac{P(B_1|A) \cdot P(A)}{P(B_1)} \]

3. **将后验概率 \( P(A|B_1) \) 作为新的先验概率**：
   - 新的先验概率 \( P(A) \) 更新为 \( P(A|B_1) \)。

4. **收集下一轮证据 \( B_2 \)**：
   - 使用更新后的先验概率 \( P(A) = P(A|B_1) \)，计算第二轮证据 \( B_2 \) 下的后验概率 \( P(A|B_2) \)：
   \[ P(A|B_1 \cap B_2) = \frac{P(B_2|A) \cdot P(A|B_1)}{P(B_2|B_1)} \]

   注意，这里 \( P(B_2|B_1) \) 是条件概率，表示在已知 \( B_1 \) 发生的情况下 \( B_2 \) 发生的概率。

这个过程可以持续进行，随着越来越多的证据被加入，每一轮的后验概率都会成为下一轮的先验概率，从而不断更新对事件发生概率的估计。

### 示例

假设你正在诊断疾病，每次都有新的测试结果。

#### 初始先验概率 \( P(A) \)
假设某人患有疾病的初始概率为 \( P(A) = 0.01 \)（即1%）。

#### 第一轮证据 \( B_1 \)
第一次测试结果为阳性，假设 \( P(B_1|A) = 0.99 \)（测试敏感性为99%），假阳性率 \( P(B_1|\neg A) = 0.05 \)。

计算后验概率：
\[ P(A|B_1) = \frac{P(B_1|A) \cdot P(A)}{P(B_1)} \]
\[ P(B_1) = P(B_1|A)P(A) + P(B_1|\neg A)P(\neg A) \]
\[ = 0.99 \cdot 0.01 + 0.05 \cdot 0.99 \]
\[ = 0.0099 + 0.0495 = 0.0594 \]
\[ P(A|B_1) = \frac{0.99 \cdot 0.01}{0.0594} = \frac{0.0099}{0.0594} \approx 0.166 \]

#### 第二轮证据 \( B_2 \)
第二次测试结果为阳性，再次使用相同的测试敏感性和假阳性率。

更新先验概率：
\[ P(A) = P(A|B_1) \approx 0.166 \]

计算新的后验概率：
\[ P(A|B_1 \cap B_2) = \frac{P(B_2|A) \cdot P(A|B_1)}{P(B_2|B_1)} \]
\[ P(B_2|B_1) = P(B_2|A)P(A|B_1) + P(B_2|\neg A)P(\neg A|B_1) \]
\[ = 0.99 \cdot 0.166 + 0.05 \cdot (1 - 0.166) \]
\[ = 0.16434 + 0.0417 = 0.20604 \]
\[ P(A|B_1 \cap B_2) = \frac{0.99 \cdot 0.166}{0.20604} \approx 0.797 \]

因此，经过两轮阳性测试后，后验概率表明这个人患有疾病的概率约为79.7%。

这个示例展示了如何通过递归贝叶斯更新，逐步整合新的证据来更新对事件的概率估计。
